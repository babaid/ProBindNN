
@misc{you_position-aware_2019,
	title = {Position-aware {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1906.04817},
	abstract = {Learning node embeddings that capture a node's position within the broader graph structure is crucial for many prediction tasks on graphs. However, existing Graph Neural Network (GNN) architectures have limited power in capturing the position/location of a given node with respect to all other nodes of the graph. Here we propose Position-aware Graph Neural Networks (P-GNNs), a new class of GNNs for computing position-aware node embeddings. P-GNN first samples sets of anchor nodes, computes the distance of a given target node to each anchor-set,and then learns a non-linear distance-weighted aggregation scheme over the anchor-sets. This way P-GNNs can capture positions/locations of nodes with respect to the anchor nodes. P-GNNs have several advantages: they are inductive, scalable,and can incorporate node feature information. We apply P-GNNs to multiple prediction tasks including link prediction and community detection. We show that P-GNNs consistently outperform state of the art GNNs, with up to 66\% improvement in terms of the ROC AUC score.},
	urldate = {2022-07-12},
	publisher = {arXiv},
	author = {You, Jiaxuan and Ying, Rex and Leskovec, Jure},
	month = jun,
	year = {2019},
	note = {arXiv:1906.04817 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
	annote = {Comment: ICML 2019, long oral},
	file = {arXiv Fulltext PDF:C\:\\Users\\debab\\Zotero\\storage\\SRVIK2KN\\You et al. - 2019 - Position-aware Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\debab\\Zotero\\storage\\AMITHWQP\\1906.html:text/html},
}

@book{noauthor_notitle_nodate,
}

@article{noauthor_notitle_nodate-1,
}

@article{wang_pros-gnn_2021,
	title = {{ProS}-{GNN}: {Predicting} effects of mutations on protein stability using graph neural networks},
	url = {http://biorxiv.org/content/early/2021/10/26/2021.10.25.465658.abstract},
	doi = {10.1101/2021.10.25.465658},
	abstract = {Motivation Predicting protein stability change upon variation through computational approach is a valuable tool to unveil the mechanisms of mutation-induced drug failure and help to develop immunotherapy strategies. However, some machine learning based methods tend to be overfitting on the training data or show anti-symmetric biases between direct and reverse mutations. Moreover, this field requires the methods to fully exploit the limited experimental data.Results Here we pioneered a deep graph neural network based method for predicting protein stability change upon mutation. After mutant part data extraction, the model encoded the molecular structure-property relationships using message passing and incorporated raw atom coordinates to enable spatial insights into the molecular systems. We trained the model using the S2648 and S3412 datasets, and tested on the Ssym and Myoglobin datasets. Compared to existing methods, our proposed method showed competitive high performance in data generalization and bias suppression with ultra-low time consumption. Furthermore, method was applied to predict the Pyrazinamide’s Gibbs free energy change for a real case study.Availability https://github.com/shuyu-wang/ProS-GNN.Contact vincentwang622\{at\}126.comCompeting Interest StatementThe authors have declared no competing interest.},
	journal = {bioRxiv},
	author = {Wang, Shuyu and Tang, Hongzhou and Shan, Peng and Zuo, Lei},
	month = jan,
	year = {2021},
	pages = {2021.10.25.465658},
}

@book{yao_ma_and_jiliang_tang_deep_2021,
	title = {Deep {Learning} on {Graphs}},
	url = {https://web.njit.edu/~ym329/dlg_book/dlg_book.pdf},
	publisher = {Cambridge University Press},
	author = {Yao Ma {and} Jiliang Tang},
	year = {2021},
}

@book{haykin_neural_2009,
	address = {Hamilton, Ontario, Canada},
	edition = {Third},
	title = {Neural {Networks} and {Learning} {Machines}},
	url = {http://dai.fmph.uniba.sk/courses/NN/haykin.neural-networks.3ed.2009.pdf},
	publisher = {Pearson Education, Inc.},
	author = {Haykin, Simon},
	year = {2009},
}

@article{xiong_pushing_2020,
	title = {Pushing the {Boundaries} of {Molecular} {Representation} for {Drug} {Discovery} with the {Graph} {Attention} {Mechanism}},
	volume = {63},
	issn = {0022-2623},
	url = {https://doi.org/10.1021/acs.jmedchem.9b00959},
	doi = {10.1021/acs.jmedchem.9b00959},
	number = {16},
	journal = {Journal of Medicinal Chemistry},
	author = {Xiong, Zhaoping and Wang, Dingyan and Liu, Xiaohong and Zhong, Feisheng and Wan, Xiaozhe and Li, Xutong and Li, Zhaojun and Luo, Xiaomin and Chen, Kaixian and Jiang, Hualiang and Zheng, Mingyue},
	month = aug,
	year = {2020},
	note = {Publisher: American Chemical Society},
	pages = {8749--8760},
	annote = {doi: 10.1021/acs.jmedchem.9b00959},
}

@misc{li_gated_2017,
	title = {Gated {Graph} {Sequence} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.05493},
	doi = {10.48550/arXiv.1511.05493},
	abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.},
	urldate = {2022-09-02},
	publisher = {arXiv},
	author = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
	month = sep,
	year = {2017},
	note = {arXiv:1511.05493 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Published as a conference paper in ICLR 2016. Fixed a typo},
	file = {arXiv Fulltext PDF:C\:\\Users\\debab\\Zotero\\storage\\IZ2P2HCL\\Li et al. - 2017 - Gated Graph Sequence Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\debab\\Zotero\\storage\\6ZKW6GCE\\1511.html:text/html},
}

@article{yao_depth-gated_2015,
	title = {Depth-{Gated} {Recurrent} {Neural} {Networks}},
	author = {Yao, Kaisheng and Cohn, Trevor and Vylomova, Katerina and Duh, Kevin and Dyer, Chris},
	month = aug,
	year = {2015},
}

@book{noauthor_fundamentals_nodate,
	edition = {Second},
	title = {Fundamentals of {Protein} {Structure} and {Function}},
}

@book{noauthor_fundamentals_nodate-1,
	title = {Fundamentals of {Protein} {Structure} and {Function}},
	url = {https://link.springer.com/book/10.1007/978-3-319-19920-7},
	language = {en},
	urldate = {2022-09-02},
	file = {Snapshot:C\:\\Users\\debab\\Zotero\\storage\\48PS6KM2\\978-3-319-19920-7.html:text/html},
}

@misc{noauthor_understanding_nodate,
	title = {Understanding {LSTM} {Networks} -- colah's blog},
	url = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	urldate = {2022-09-02},
	file = {Understanding LSTM Networks -- colah's blog:C\:\\Users\\debab\\Zotero\\storage\\8W7YVD6N\\2015-08-Understanding-LSTMs.html:text/html},
}

@misc{noauthor_torch_geometricnnconvgated_graph_conv_nodate,
	title = {torch\_geometric.nn.conv.gated\_graph\_conv — pytorch\_geometric documentation},
	url = {https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/gated_graph_conv.html#GatedGraphConv},
	urldate = {2022-09-02},
	file = {torch_geometric.nn.conv.gated_graph_conv — pytorch_geometric documentation:C\:\\Users\\debab\\Zotero\\storage\\UXTQYZUC\\gated_graph_conv.html:text/html},
}

@misc{noauthor_backpropagation_nodate,
	title = {Backpropagation and {SGD} · {Knet}.jl},
	url = {https://denizyuret.github.io/Knet.jl/latest/backprop/},
	urldate = {2022-09-02},
	file = {Backpropagation and SGD · Knet.jl:C\:\\Users\\debab\\Zotero\\storage\\TXKUDA4T\\backprop.html:text/html},
}

@article{boyle_molecular_2008,
	title = {Molecular biology of the cell, 5th edition by {B}. {Alberts}, {A}. {Johnson}, {J}. {Lewis}, {M}. {Raff}, {K}. {Roberts}, and {P}. {Walter}},
	volume = {36},
	issn = {1470-8175},
	url = {https://doi.org/10.1002/bmb.20192},
	doi = {10.1002/bmb.20192},
	number = {4},
	urldate = {2022-09-02},
	journal = {Biochemistry and Molecular Biology Education},
	author = {Boyle, John},
	month = jul,
	year = {2008},
	note = {Publisher: John Wiley \& Sons, Ltd},
	pages = {317--318},
	annote = {https://doi.org/10.1002/bmb.20192},
}

@misc{ward_practical_2021,
	title = {A {Practical} {Tutorial} on {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2010.05234},
	doi = {10.48550/arXiv.2010.05234},
	abstract = {Graph neural networks (GNNs) have recently grown in popularity in the field of artificial intelligence (AI) due to their unique ability to ingest relatively unstructured data types as input data. Although some elements of the GNN architecture are conceptually similar in operation to traditional neural networks (and neural network variants), other elements represent a departure from traditional deep learning techniques. This tutorial exposes the power and novelty of GNNs to AI practitioners by collating and presenting details regarding the motivations, concepts, mathematics, and applications of the most common and performant variants of GNNs. Importantly, we present this tutorial concisely, alongside practical examples, thus providing a practical and accessible tutorial on the topic of GNNs.},
	urldate = {2022-09-02},
	publisher = {arXiv},
	author = {Ward, Isaac Ronald and Joyner, Jack and Lickfold, Casey and Guo, Yulan and Bennamoun, Mohammed},
	month = dec,
	year = {2021},
	note = {arXiv:2010.05234 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted at ACM CSUR},
	file = {arXiv Fulltext PDF:C\:\\Users\\debab\\Zotero\\storage\\VIX5IGGP\\Ward et al. - 2021 - A Practical Tutorial on Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\debab\\Zotero\\storage\\VKTDQXDG\\2010.html:text/html},
}

@article{cao_deepddg_2019,
	title = {{DeepDDG}: {Predicting} the {Stability} {Change} of {Protein} {Point} {Mutations} {Using} {Neural} {Networks}},
	volume = {59},
	issn = {1549-9596},
	url = {https://doi.org/10.1021/acs.jcim.8b00697},
	doi = {10.1021/acs.jcim.8b00697},
	number = {4},
	journal = {Journal of Chemical Information and Modeling},
	author = {Cao, Huali and Wang, Jingxue and He, Liping and Qi, Yifei and Zhang, John Z.},
	month = apr,
	year = {2019},
	note = {Publisher: American Chemical Society},
	pages = {1508--1514},
	annote = {doi: 10.1021/acs.jcim.8b00697},
}

@misc{noauthor_pytorch_nodate,
	title = {{PyTorch}},
	url = {https://www.pytorch.org},
	abstract = {An open source machine learning framework that accelerates the path from research prototyping to production deployment.},
	language = {en},
	urldate = {2022-09-02},
	file = {Snapshot:C\:\\Users\\debab\\Zotero\\storage\\TUV7WNDH\\pytorch.org.html:text/html},
}

@article{zhou_graph_2020,
	title = {Graph neural networks: {A} review of methods and applications},
	volume = {1},
	issn = {2666-6510},
	url = {https://www.sciencedirect.com/science/article/pii/S2666651021000012},
	doi = {10.1016/j.aiopen.2021.01.001},
	abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.},
	journal = {AI Open},
	author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	month = jan,
	year = {2020},
	keywords = {Deep learning, Graph neural network},
	pages = {57--81},
}

@misc{menzli_graph_2020,
	title = {Graph {Neural} {Network} and {Some} of {GNN} {Applications}: {Everything} {You} {Need} to {Know}},
	shorttitle = {Graph {Neural} {Network} and {Some} of {GNN} {Applications}},
	url = {https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications},
	abstract = {The recent success of neural networks has boosted research on pattern recognition and data mining.  Machine learning tasks, like object detection, machine translation, and speech recognition, have been given new life with end-to-end deep learning paradigms like CNN, RNN, or autoencoders.  Deep Learning is good at capturing hidden patterns of Euclidean data (images, text, videos).  […]},
	language = {en-US},
	urldate = {2022-09-02},
	journal = {neptune.ai},
	author = {Menzli, Amal},
	month = nov,
	year = {2020},
	file = {Snapshot:C\:\\Users\\debab\\Zotero\\storage\\RDN9VSY9\\graph-neural-network-and-some-of-gnn-applications.html:text/html},
}

@article{frenz_prediction_2020,
	title = {Prediction of {Protein} {Mutational} {Free} {Energy}: {Benchmark} and {Sampling} {Improvements} {Increase} {Classification} {Accuracy}},
	volume = {8},
	issn = {2296-4185},
	shorttitle = {Prediction of {Protein} {Mutational} {Free} {Energy}},
	url = {https://www.frontiersin.org/articles/10.3389/fbioe.2020.558247},
	abstract = {Software to predict the change in protein stability upon point mutation is a valuable tool for a number of biotechnological and scientific problems. To facilitate the development of such software and provide easy access to the available experimental data, the ProTherm database was created. Biases in the methods and types of information collected has led to disparity in the types of mutations for which experimental data is available. For example, mutations to alanine are hugely overrepresented whereas those involving charged residues, especially from one charged residue to another, are underrepresented. ProTherm subsets created as benchmark sets that do not account for this often underrepresent tense certain mutational types. This issue introduces systematic biases into previously published protocols’ ability to accurately predict the change in folding energy on these classes of mutations. To resolve this issue, we have generated a new benchmark set with these problems corrected. We have then used the benchmark set to test a number of improvements to the point mutation energetics tools in the Rosetta software suite.},
	urldate = {2022-09-02},
	journal = {Frontiers in Bioengineering and Biotechnology},
	author = {Frenz, Brandon and Lewis, Steven M. and King, Indigo and DiMaio, Frank and Park, Hahnbeom and Song, Yifan},
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\debab\\Zotero\\storage\\HNVSAE4R\\Frenz et al. - 2020 - Prediction of Protein Mutational Free Energy Benc.pdf:application/pdf},
}

@misc{noauthor_welcome_nodate,
	title = {Welcome to {Graphein}’s documentation!},
	url = {https://graphein.ai/index.html},
	abstract = {This package provides functionality for producing a number of types of graph-based representations of proteins. We provide compatibility with standard geometric deep learning library formats (curre...},
	urldate = {2022-09-02},
	file = {Snapshot:C\:\\Users\\debab\\Zotero\\storage\\2SIGUSQS\\graphein.ai.html:text/html},
}

@misc{noauthor_pymol_nodate,
	title = {{PyMOL} {\textbar} pymol.org},
	url = {https://pymol.org/2/},
	urldate = {2022-09-02},
	file = {PyMOL | pymol.org:C\:\\Users\\debab\\Zotero\\storage\\GL6LLSJ8\\2.html:text/html},
}

@misc{scellier_equivalence_2018,
	title = {Equivalence of {Equilibrium} {Propagation} and {Recurrent} {Backpropagation}},
	url = {http://arxiv.org/abs/1711.08416},
	abstract = {Recurrent Backpropagation and Equilibrium Propagation are supervised learning algorithms for fixed point recurrent neural networks which differ in their second phase. In the first phase, both algorithms converge to a fixed point which corresponds to the configuration where the prediction is made. In the second phase, Equilibrium Propagation relaxes to another nearby fixed point corresponding to smaller prediction error, whereas Recurrent Backpropagation uses a side network to compute error derivatives iteratively. In this work we establish a close connection between these two algorithms. We show that, at every moment in the second phase, the temporal derivatives of the neural activities in Equilibrium Propagation are equal to the error derivatives computed iteratively by Recurrent Backpropagation in the side network. This work shows that it is not required to have a side network for the computation of error derivatives, and supports the hypothesis that, in biological neural networks, temporal derivatives of neural activities may code for error signals.},
	urldate = {2022-09-04},
	publisher = {arXiv},
	author = {Scellier, Benjamin and Bengio, Yoshua},
	month = may,
	year = {2018},
	note = {arXiv:1711.08416 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\debab\\Zotero\\storage\\D7HSE2MH\\Scellier and Bengio - 2018 - Equivalence of Equilibrium Propagation and Recurre.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\debab\\Zotero\\storage\\M6GZ64LR\\1711.html:text/html},
}
